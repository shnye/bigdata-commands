## 基础平台选型

- Apache ：版本迭代快，可拓展性强，需要考虑兼容性。
- Cloudera：稳定，更新慢，付费。
- Hortonworks：Ambari 免费开源稳定，使用较少。



## 操作系统调优

- 调节操作系统文件描述符的上限：ulimit -a 查看 调整为65536
- 设置socket（listen）的backlog上限（默认128）：echo 4096 >/proc/sys/net/core/somaxconn
- 调节操作系统swap的比例（默认设置值为60，物理内存使用到40时则开始使用）：/proc/sys/vm/swappiness 设置为0-10之间。
- 禁用THP功能（提高内存性能，但是会提高cpu的占用率）： 关闭





## 存储平台技术选型——Kafka

### 版本选择

#### 常用版本特点

- 0.8	标志着kafka成熟，开始商用
- 0.9	安全模块，偏移量存储架构升级（从存ZK（zk不适合高频读写），转成存在kafka _comsumer_offset中）
- 0.10	提供流式计算的能力，生产者优化（bacth提交max(16K,context.size)),机架感知
- 0.11	生产者幂等事务支持
- 1.X	重新定位 A distributed streaming platform
- 2.X	优化了stream功能，安全粒度更细



**注意：0.11以后的版本，但高版本需要考虑兼容性**



### Kafka 资源评估

背景：每天需要搞定10亿的请求 



QPS估算：28法则。80%的数据会在白天涌入，其中80%数据会在白天的20%的时间涌入

所以需要3小时处理6.4亿个请求，则QPS为6万

台数评估：如果资源充足，高峰期QPS控制集群的30%左右，所以集群需要承载的QPS为20万左右比较安全，根据经验1台服务器（8核心）可以支撑4万QPS，所以需要物理机5台。在再加上消费者的请求 大概1.5倍 需要7-8台服务器。

存储估算：每天十亿条，每个请求10K（不大不小）总9T的数据 保存3个副本 27T/天 保留最近5天的数据，故需要27 * 5 = 135T的数据。7台服务器，每台服务器一般是11块硬盘，一个盘2T即可，还需要磁盘预留建议3T盘即可。目录的配置也需要配置多目录，增加磁盘同步读写性能。

SSD还是SAS？：SSD主要是快在随机读写，kafka是顺序读写，机械硬盘顺序读写和内存随机读写性能差不多，对于kafka来说SAS即可，当然有条件上SSD肯定更好。

内存计算：假设有300个topic 每个有3个副本，每个topic有3个partition 每个partition log文件为1G，所以需要2700G 目前有7台服务器每台平均需要400-500G内存，其中10-20%的数据在内存中性能就很好了所以需要40-100G 加上本身需要的运行内存 kafka服务器的需要64/128G内存即可（建议128G）。

CPU计算：kafka进程里大概有多少线程？

- ​	accept线程

- ​	默认3个process线程（一般设置9个）

- ​	默认的8RequestHandle线程（可以设置成32个）

- ​	清理日志的线程

- ​	感知Controller的线程

- ​	副本同步的线程

- ​    ......

  估算下来内部有100多个线程

  根据：4个CPU核心，一般来说可以应对几十个线程。8个cpu核可以支撑几十个线程繁忙工作，所以建议16核心，基本上可以hold的住100-200个线程繁忙工作，当然32个核心会更好

  

#### 总结

10亿写请求，6w/s吞吐量，9T的数据，7台物理机

硬盘：11（SAS） * 2T 7200转

内存：128G JVM分配6G 其余都给OS cache

CPU：16/32核心

网络：万兆网卡



### 核心配置

log.retention.hours=72 日志保留时间建议3-5天

log.segment.bytes=1073741824 日志文件大小建议配置1G

log.flush.interval.messages=10000

log.flush.interval.ms=1000 当写入1万条或者1秒达到其中一个即刷写到硬盘

log.dirs=/data1,/data2,/data3 多磁盘配置，提高读写并发能力

num.io,threads = 8 根据数据量大小适当提高3-4倍 （32）

Num.network.threads = 3 根据数据量大小适当提高3-4倍（9）

## 存储平台技术选型——HDFS

### 版本选择

#### 常用版本特点

- 1.x  稳定版本，标志hadoop成熟走向商用
- 2.0 支持高可用，支持联邦
- 2.7 流行度比较广，比较稳定，bug持续修复
- 3.X 支持多个NameNode的HA，纠删码



关于纠删码：解决多个副本空间占用问题。 每个文件块会分出一部分空间存储校验数据。如果其中某个分片丢失。利用其他文件块存储的校验信息通过矩阵的运算把丢失的块的数据恢复回来。

纠删码缺点：数据稳定性较差（提交的任务不一定能提交到存储该数据的节点，需要多一步数据复制的过程）

使用场景：冷数据存储，也可以使用2个副本 + 纠删码功能配合使用



联邦与高可用的选择：分界线1000台服务器，尽量不使用。





**注意：（推荐使用2.7.5以后的版本)**



### NameNode 配置设置

#### 内存

运行在一台独立的服务器128G内存（尽量越大越好 256G）

理论上最大可支持1000台 * 20T * 80%使用率 服务器

修改hadoop-env.sh 进行配置

export HADOOP_HEAPSIZE_MAX = 80%memory（90-100G）

export HADOOP_HEAPSIZE_MIN = 80%memory（90-100G）



#### 关键参数配置

dfs.replacation 默认设置为3，不建议修改

dfs.block.size 默认为128M 对于数据量大的集群，设置为256或者512M（根据分布设置为数量最多的大小偏小一点）

dfs.datanode.data.dir 分布在各个独立的磁盘上，充分利用节点的IO读写能力

dfs.datanode.max.transfer.threads 配置datanode可以同时处理的最大文件数量，建议调大，最大可以配置到65535



### DataNode 配置设置

#### 内存

修改hadoop-env.sh 进行配置

export HDFS_DATANODE_HEAPSIZE=4096

export HDFS_DATANODE_OPTS="-Xms${HDFS_DATANODE_HEAPSIZE}m -Xmx${HDFS_DATANODE_HEAPSIZE}m" 

建议DataNode堆内存设置在4G-8G 即可，更多的内存留给YARN 



## 资源调度yarn配置

Nodemanager 节点配置80-85%的内存资源

```xml
<property>
	<name>yarn.nodemanager.resource.memory-mb</name> 
	<value>102400</value>
</property>
<property>
	<name>yarn.nodemanager.resource.cpu-vcores</name> 
	<value>32</value>
</property>
```





## 服务器的规划

### 总览

小于500台/500-100台

Zookeeper 3-5个节点

NameNode HA 2个节点

JournalNode 3-5个节点



### 网络设计



#### 网络总览

双总交换机 + 多机架

双交换机（HA），双网卡，双电源 为一个机架



#### 机架感知

方法：百度脚本（很简单，待补充）

作用：保证有一个副本在其他机架上，避免单机架宕机导致数据不可用。



### 硬件选择

#### 厂商选择

决定不了了解即可

IBM：几乎不会有问题，比较贵，售后好

惠普：中规中矩

戴尔：中规中矩

浪潮：性价比高，出问题概率较大



#### 配置选择

CPU：推荐4路32核等（4*8），主频至少2-2.5GhHz

内存：推荐64-256（建议选择大内存）

磁盘：分为2组，系统盘1-2T * 2 做raid1，数据盘2-10T（SSD,SAS）NameNode尽量使用SSD

（HDFS频繁读写，会存在磁盘损坏的情况，不建议单盘太大，损坏恢复成本大）

网卡：万兆网卡（光纤卡）

电源：均配置冗余电源，有条件的可以具备发电能力





## 案例：预估一年的存储

原则：存储+ 计算 + 可扩展余地（机柜位置，网络接口，磁盘接口等）

1.每天的数据为9T，副本数3，一年的存储资源为 9 * 3 *365 = 9855T

2.数据的加工建模：9855 * 3 = 29565T

3.数据增速是每年50% 29565 *1.5 = 44347T

4.磁盘使用最大为80%  44347/0.8 = 55434T

5.压缩比50% 故需要55434 * 0.5 = 27717T



机器配置 32核 128G内存 11*7 T

故需要 27717/77 = 359台服务器





## 常见问题

1.下线DataNode

2.磁盘故障

3.yarn标记为不健康

4.磁盘存储不均衡

5.missing blocks

6.broker运行日志大量topic不存在报错，导致节点不可用

7.kafka broker oom

8.kafka topic扩分区

