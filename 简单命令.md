# Kafka

```shell
#查看topic
bin/kafka-topics.sh --zookeeper 10.10.10.75:2181 --list
```

```shell
#创建topic
bin/kafka-topics.sh --zookeeper 8.134.123.119:2181 --create --replication-factor 1 --partitions 1 --topic test
```

```shell
#删除topic
bin/kafka-topics.sh --delete --zookeeper hadoop119:2181 --topic topic_zeppelin
```

```shell
#创建生产者
bin/kafka-console-producer.sh \
--broker-list 10.10.10.75:9092 --topic order_withdraw_create_topic
```

```shell
#创建消费者
bin/kafka-console-consumer.sh \
--bootstrap-server 10.10.10.75:9092  --topic order_withdraw_create_topic --from-beginning
```

```shell
#查看topic信息
bin/kafka-topics.sh --zookeeper 10.10.10.75:9092 \
--describe --topic order_paid_topic
```

```shell
#producer压测
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 acks=-1

#日志形式:
97037 records sent, 19403.5 records/sec (3.70 MB/sec), 2447.1 ms avg latency, 3077.0 max latency.

#comsumer压测
bin/kafka-consumer-perf-test.sh --zookeeper hadoop102:2181 --topic test --fetch-size 10000 --messages 10000000 --threads 1
```

```shell
#消费者组查看
bin/kafka-consumer-groups.sh --describe --group iaas  --bootstrap-server 10.10.10.75:9092
```

```shell
#kafkamanager使用

#下载编译好的:
kafka-manager-1.3.3.7.zip。
链接：https://pan.baidu.com/s/1qYifoa4 密码：el4o


#解压
unzip kafka-manager-1.3.3.7.zip -d /training/
cd /training/kafka-manager-1.3.3.7



#2.3.修改配置 conf/application.properties
编辑配置文件application.conf
#kafka-manager.zkhosts="localhost:2181" ##注释这一行，下面添加一行,即zk集群地址.
kafka-manager.zkhosts="192.168.85.111:2181,192.168.85.111:2182,192.168.85.111:2183"


#启动kafkamanager(自己指定端口).
nohup ./bin/kafka-manager -Dconfig.file=./conf/application.conf -Dhttp.port=9000 &

(kafka-manager 默认的端口是9000，可通过 -Dhttp.port，指定端口; 
-Dconfig.file=conf/application.conf指定配置文件:)



#然后去浏览器输入端口:192.168.85.111:9000,就可以进入kafkamanager页面.
```



# Flume

```shell
#TailDir 样例

a1.sources=r1
a1.channels=c1 c2

# configure source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume/test/log_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
a1.sources.r1.fileHeader = true
a1.sources.r1.channels = c1 c2

#interceptor
a1.sources.r1.interceptors =  i1 i2
a1.sources.r1.interceptors.i1.type = com. mufeng.flume.interceptor.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com. mufeng.flume.interceptor.LogTypeInterceptor$Builder

a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.parseAsFlumeEvent = false
a1.channels.c1.kafka.consumer.group.id = flume-consumer

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false
a1.channels.c2.kafka.consumer.group.id = flume-consumer
```



```shell
#flume启动
bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties
```



# Docker

```shell
docker images #查看所有本地主机上的镜像 可以使用docker image ls代替

docker search #搜索镜像

docker pull #下载镜像 docker image pull

docker rmi -f 镜像id #删除指定id的镜像

docker rmi -f $(docker images -aq) #删除全部的镜像
```



## 新建容器并启动

```shell
docker run [可选参数] image | docker container run [可选参数] image

docker run -it centos /bin/bash

#参书说明
--name="Name"		#容器名字 tomcat01 tomcat02 用来区分容器
-d					#后台方式运行
-it 				#使用交互方式运行，进入容器查看内容
-p					#指定容器的端口 -p 8080(宿主机):8080(容器)
		-p ip:主机端口:容器端口
		-p 主机端口:容器端口(常用)
		-p 容器端口
		容器端口
-P(大写) 				随机指定端口
```



## 列出所有运行的容器

```shell
docker ps 命令  		#列出当前正在运行的容器
  -a, --all     	 #列出当前正在运行的容器 + 带出历史运行过的容器
  -n=?, --last int   #列出最近创建的?个容器 ?为1则只列出最近创建的一个容器,为2则列出2个
  -q, --quiet        #只列出容器的编号
```



## 启动和停止容器的操作

```shell
docker start 容器id	#启动容器
docker restart 容器id	#重启容器
docker stop 容器id	#停止当前正在运行的容器
docker kill 容器id	#强制停止当前容器
1234
```





## 删除容器

```shell
docker rm 容器id   				#删除指定的容器，不能删除正在运行的容器，如果要强制删除 rm -rf
docker rm -f $(docker ps -aq)  	 #删除所有的容器
docker ps -a -q|xargs docker rm  #删除所有的容器
123
```



## 查看镜像的元数据日志

```shell
# 命令
docker inspect 容器id

docker logs -t --tail n 容器id #查看n行日志
docker logs -ft 容器id #跟着日志
```



## 进入容器 

```shell
docker exec -it 容器id /bin/bash

docker cp 容器id:容器内路径  主机目的路径
```



## commit镜像

```shell
docker commit 提交容器成为一个新的副本

# 命令和git原理类似
docker commit -m="描述信息" -a="作者" 容器id 目标镜像名:[版本TAG]
```



## 映射、容器卷

```shell
docker run -it -v 主机目录:容器内目录  -p 主机端口:容器内端口
docker run -it -v /home/ceshi:/home centos /bin/bash
```

## DockerFile的指令

```shell
FROM				# from:基础镜像，一切从这里开始构建
MAINTAINER			# maintainer:镜像是谁写的， 姓名+邮箱
RUN					# run:镜像构建的时候需要运行的命令
ADD					# add:步骤，tomcat镜像，这个tomcat压缩包！添加内容 添加同目录
WORKDIR				# workdir:镜像的工作目录
VOLUME				# volume:挂载的目录
EXPOSE				# expose:保留端口配置
CMD					# cmd:指定这个容器启动的时候要运行的命令，只有最后一个会生效，可被替代
ENTRYPOINT			# entrypoint:指定这个容器启动的时候要运行的命令，可以追加命令
ONBUILD				# onbuild:当构建一个被继承DockerFile这个时候就会运行onbuild的指令，触发指令
COPY				# copy:类似ADD，将我们文件拷贝到镜像中
ENV					# env:构建的时候设置环境变量！

构建
#因为dockerfile命名使用默认命名 因此不用使用-f 指定文件
docker build -t 名字:版本号 .

#-d:后台运行 -p:暴露端口 --name:别名 -v:绑定路径 
docker run -d -p 8080:8080 --name tomcat01
```



## 发布镜像

```shell
docker login -u 你的用户名 -p 你的密码

# 会发现push不上去，因为如果没有前缀的话默认是push到 官方的library
# 解决方法：
# 第一种 build的时候添加你的dockerhub用户名，然后在push就可以放到自己的仓库了
$ docker build -t 用户名/mytomcat:0.1 .

# 第二种 使用docker tag #然后再次push
$ docker tag 容器id 用户名/mytomcat:1.0 #然后再次push
$ docker push 用户名/mytomcat:1.0


#发布到阿里云
$ sudo docker login --username=zchengx registry.cn-shenzhen.aliyuncs.com
$ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/dsadxzc/cheng:[镜像版本号]

# 修改id 和 版本
sudo docker tag a5ef1f32aaae registry.cn-shenzhen.aliyuncs.com/dsadxzc/cheng:1.0
# 修改版本
$ sudo docker push registry.cn-shenzhen.aliyuncs.com/dsadxzc/cheng:[镜像版本号]
```



## 网络

```shell
docker network create --driver overlay --attachable --subnet 10.10.0.0/16 --gateway 10.10.0.1 healthApp
```



## Docker Swarm

```shell
docker swarm init --advertise-addr 主机ip
```



## Docker Stack

```shell
docker stack deploy -c flink.yml flink

docker stack rm flink
```



## Docker容器日志

```shell
$ docker logs [OPTIONS] CONTAINER
  Options:
        --details        显示更多的信息
    -f, --follow         跟踪实时日志
        --since string   显示自某个timestamp之后的日志，或相对时间，如42m（即42分钟）
        --tail string    从日志末尾显示多少行日志， 默认是all
    -t, --timestamps     显示时间戳
        --until string   显示自某个timestamp之前的日志，或相对时间，如42m（即42分钟）
```

例子：

查看指定时间后的日志，只显示最后100行：

```shell
$ docker logs -f -t --since="2018-02-08" --tail=100 CONTAINER_ID
```

查看最近30分钟的日志:

```shell
$ docker logs --since 30m CONTAINER_ID
```

查看某时间之后的日志：

```shell
$ docker logs -t --since="2018-02-08T13:23:37" CONTAINER_ID
```

查看某时间段日志：

```shell
$ docker logs -t --since="2018-02-08T13:23:37" --until "2018-02-09T12:23:37" CONTAINER_ID
```





# Flink

nohup ./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 &



# Spark

```shell
#本地模式，不启动spark集群也能运行
$SPARK_HOME/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[4] \
--driver-memory 512M \
--executor-memory 512M \
--total-executor-cores 1 \
$SPARK_HOME/examples/jars/spark-examples_2.11-2.4.5.jar \
10
```



```shell
#standalone 模式
$SPARK_HOME/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://bigdata02:7077,bigdata04:7077 \
--driver-memory 512M \
--executor-memory 512M \
--total-executor-cores 1 \
$SPARK_HOME/examples/jars/spark-examples_2.11-2.4.5.jar \
100
```



```shell
#yarn client模式
$SPARK_HOME/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
--driver-memory 512M \
--executor-memory 512M \
--total-executor-cores 1 \
$SPARK_HOME/examples/jars/spark-examples_2.11-2.4.5.jar \
10
```

```shell
#yarn class模式
$SPARK_HOME/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \ #指定主类
--master yarn \ #yarn模式
--num-executors 1 \
--deploy-mode cluster \ 
--driver-memory 512M \
--executor-memory 512M \
--total-executor-cores 1 \
--------------可选-------------
--conf spark.default.parallelism=2 \ #该参数用于设置每个stage的默认task数量。
--conf spark.executor.instances=6 \ #等同于--num-executors
--conf spark.executor.memoryOverhead=4G \ #默认是executorMemory的10%
--conf spark.storage.memoryFraction=0.5 \ #该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。
--conf spark.shuffle.memoryFraction=0.3 \ #于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作
时能够使用的Executor内存的比例，默认是0.2。
--conf spark.sql.shuffle.partitions=400 \ #未知
$SPARK_HOME/examples/jars/spark-examples_2.11-2.4.5.jar \ #指定jar包
10 #主类参数
```



# Zookeeper 



命令作用

```shell
ls /   ls /ZooKeeper 查看znode子节点列表

create /zk "myData"  创建znode节点

get /zk   get /zk/node1 获取znode数据

set /zk "myData1" 设置znode数据

ls /zk watch  就对一个节点的子节点变化事件注册了监听

get /zk watch 就对一个节点的数据内容变化事件注册了监听

create -e /zk "myData"  创建临时znode节点

create -s /zk "myData" 创建顺序znode节点

create -e -s /zk "myData" 创建临时的顺序znode节点

delete /zk  只能删除没有子znode的znode

rmr /zk  不管里头有多少znode，统统删除

status /zk  查看/zk节点的状态信息)
```





# git

清除用户名密码：

```bash
git config --global credential.helper wincred
git credential-manager uninstall
```

保存用户秘密码：

```bash
git config --global credential.helper store
```





# Mongo

创建对于该库有所有权限的用户

```shell
use food;
db.createUser({user: "food", pwd: "Ylz_dxjk&2021", roles: [{ role: "dbOwner", db: "food" }]})
```





# MySQL

## 1.mysql查看表结构

```sql
#全部内容
SELECT t.TABLE_NAME, t.TABLE_COMMENT, c.COLUMN_NAME, c.COLUMN_TYPE, c.COLUMN_COMMENT
FROM information_schema.TABLES t, INFORMATION_SCHEMA.Columns c
WHERE c.TABLE_NAME = t.TABLE_NAME
	AND t.`TABLE_SCHEMA` = 'shaxian'

#表
SELECT TABLE_NAME, TABLE_COMMENT
FROM information_schema.TABLES
WHERE table_schema = 'shaxian';

#字段
SELECT
	COLUMN_NAME 字段名称,
	COLUMN_COMMENT 描述
FROM
	INFORMATION_SCHEMA. COLUMNS
WHERE
	table_schema = 'shaxian'
AND table_name = 't_org_info'
```

# oracle

## 1.获取当前时间

```sql
to_char(sysdate,'yyyy-mm-dd')
```



## 2.判断字符串是否可以转换成日期

```sql
CREATE OR REPLACE FUNCTION IS_DATE(
    IN_DATE IN VARCHAR2)
  RETURN INTEGER
AS  val date;
begin
val:= to_date(nvl(replace(replace(IN_DATE,'-'),'/'),'a'),'yyyymmdd');
return 0;
exception when others then
return 1;
end;
```



## 3获取表字段信息



```sql
select table_name,column_name,comments from user_col_comments where table_name='his_t_patient_info'
```



# sql-server

## 查询表结构

```sql
--sql-server查询表结构
SELECT  CASE WHEN col.colorder = 1 THEN obj.name
                  ELSE ''
             END AS 表名,
        col.colorder AS 序号 ,
        col.name AS 列名 ,
        ISNULL(ep.[value], '') AS 列说明 ,
        t.name AS 数据类型 ,
        col.length AS 长度 ,
        ISNULL(COLUMNPROPERTY(col.id, col.name, 'Scale'), 0) AS 小数位数 ,
        CASE WHEN COLUMNPROPERTY(col.id, col.name, 'IsIdentity') = 1 THEN '1'
             ELSE ''
        END AS 标识 ,
        CASE WHEN EXISTS ( SELECT   1
                           FROM     dbo.sysindexes si
                                    INNER JOIN dbo.sysindexkeys sik ON si.id = sik.id
                                                              AND si.indid = sik.indid
                                    INNER JOIN dbo.syscolumns sc ON sc.id = sik.id
                                                              AND sc.colid = sik.colid
                                    INNER JOIN dbo.sysobjects so ON so.name = si.name
                                                              AND so.xtype = 'PK'
                           WHERE    sc.id = col.id
                                    AND sc.colid = col.colid ) THEN '1'
             ELSE ''
        END AS 主键 ,
        CASE WHEN col.isnullable = 1 THEN '1'
             ELSE ''
        END AS 允许空 ,
        ISNULL(comm.text, '') AS 默认值
FROM    dbo.syscolumns col
        LEFT  JOIN dbo.systypes t ON col.xtype = t.xusertype
        inner JOIN dbo.sysobjects obj ON col.id = obj.id
                                         AND obj.xtype = 'U'
                                         AND obj.status >= 0
        LEFT  JOIN dbo.syscomments comm ON col.cdefault = comm.id
        LEFT  JOIN sys.extended_properties ep ON col.id = ep.major_id
                                                      AND col.colid = ep.minor_id
                                                      AND ep.name = 'MS_Description'
        LEFT  JOIN sys.extended_properties epTwo ON obj.id = epTwo.major_id
                                                         AND epTwo.minor_id = 0
                                                         AND epTwo.name = 'MS_Description'
WHERE   obj.name = 'IN_TJ_DAT_BRXX'
ORDER BY col.colorder ;

```



# scala



## scala读取配置文件

```scala
val properties = new Properties()
properties.load(inReader)
properties.stringPropertyNames().asScala.map { k => (k, trimExceptCRLF(properties.getProperty(k))) }.toMap
```
